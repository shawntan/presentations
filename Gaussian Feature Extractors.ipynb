{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Gaussian Filters\n",
    "\n",
    "We want to learn feature extractors for the input in terms of Gaussian filters with varying width height and rotations.\n",
    "\n",
    "Let the Gaussian function be,\n",
    "\n",
    "$$ g(\\mathbf{x};\\theta,\\boldsymbol{\\sigma},\\boldsymbol{\\mu}) = \\exp\\left(-{\\left\\| \\frac{R(\\theta)}{\\boldsymbol{\\sigma}} (\\mathbf{x} - \\boldsymbol{\\mu}) \\right\\|}^2\\right) $$\n",
    "\n",
    "where $R(\\theta)$ is the rotation matrix for $\\theta$.\n",
    "\n",
    "\n",
    "Then we define a weight matrix $\\mathbf{W}$ between layers such that,\n",
    "\n",
    "$$ \\underbrace{\\mathbf{W}}_{(n,h)} = \\underbrace{\\mathbf{G}}_{(n,k)} \\underbrace{\\mathbf{M}}_{(k,h)}$$\n",
    "\n",
    "where $\\mathbf{M}$ is a standard transformation matrix freely tuned by gradient descent, and $\\mathbf{G}$ is a matrix with each column representing a Gaussian filter:\n",
    "$$\\mathbf{G}_{i,j} = g(\\left[\\text{row}(i),\\text{col}(i)\\right];\\theta_j,\\boldsymbol{\\sigma}_j,\\boldsymbol{\\mu}_j)$$\n",
    "\n",
    "\n",
    "where $\\text{row}(i)$ and $\\text{col}(i)$ give the row and column of input $i$. As a result, the free variables for tuning the matrix $\\mathbf{G}$ are then the mean/centre (2 dimensional), scale (2 dimensional), and rotation.\n",
    "\n",
    "In this way, $\\mathbf{W}$ represents a layer in which features are captured in the form of linear combinations of Gaussian shaped feature extractors from the previous layer, which is assumed to have some form of two-dimensional topology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MNIST task\n",
    "We compare the unconstrained version of a single hidden layer neural network against a neural network that learns gaussian feature extractors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['savefig.dpi'] = 3 * matplotlib.rcParams['savefig.dpi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original unconstrained form of the learner extracts blob-like structures from the image, which suggests that constraining that extracting gaussian shaped features and learning representations in terms of them might be a good fit for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 40 gaussian components in our initial experiments, and attained a 1.9% error rate on the test set for MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'mixtures.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-264fa217b94d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mgaussians\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mixtures.data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mgaussians\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgaussians\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'mixtures.data'"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "rows = 5\n",
    "[gaussians,weights] = pickle.load(open('mixtures.data'))\n",
    "gaussians = gaussians.T\n",
    "weights = weights.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below is of the 40 gaussian components learnt to extract data from the MNIST dataset. This is the matrix $\\mathbf{G}$ as mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gaussians = gaussians.reshape((rows,gaussians.shape[0]/rows,28,28))\n",
    "plt.axis('off')\n",
    "plt.imshow(np.hstack(np.hstack(gaussians)),interpolation=\"nearest\",cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we multiply this with the matrix $\\mathbf{M}$ we get a matrix which extracts weighted sums of gaussians from $\\mathbf{G}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = weights.reshape((20,25,28,28))\n",
    "plt.axis('off')\n",
    "plt.imshow(np.hstack(np.hstack(weights)),interpolation=\"nearest\",cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new system performs at 1.9% error rate, and you can see from the weights, similar sorts of feature detectors are learnt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an autoencoder\n",
    "\n",
    "In order to train the autoencoder, we simply use the transpose of $\\mathbf{W}$ for the 2nd layer, and train it using KL-divergence. We get relatively good reconstructions using 40 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gaussian import gaussian\n",
    "from theano_toolkit.parameters import Parameters\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "\n",
    "P = Parameters()\n",
    "x = T.matrix('x')    # the data is presented as rasterized images\n",
    "input_size = 28 * 28\n",
    "components = 40\n",
    "hidden_size = 500\n",
    "\n",
    "rng = np.random.RandomState(1234)\n",
    "P.W_gaussians_hidden  = 4 * np.asarray(rng.uniform(\n",
    "                    low = -np.sqrt(6. / (components + hidden_size)),\n",
    "                    high=  np.sqrt(6. / (components + hidden_size)),\n",
    "                    size=(components, hidden_size)),\n",
    "                dtype=theano.config.floatX)\n",
    "P.b_hidden = np.zeros((hidden_size,), dtype=theano.config.floatX)\n",
    "P.b_recon  = np.zeros((input_size,),  dtype=theano.config.floatX)\n",
    "\n",
    "G = gaussian(P,28,28,components)\n",
    "W = T.dot(G,P.W_gaussians_hidden)\n",
    "hidden = T.nnet.sigmoid(T.dot(x,W) + P.b_hidden)\n",
    "recon  = T.nnet.sigmoid(T.dot(hidden,W.T) + P.b_recon)\n",
    "data = pickle.load(open('mnist.pkl'))\n",
    "get_filters = theano.function(inputs=[],outputs=[G,W])\n",
    "f = theano.function(inputs=[x],outputs=recon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P.load('ae_gaussian.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the reconstructions of several examples from the validation set, and visualise the matrices of $\\mathbf{G}$ and $\\mathbf{W}$ again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images_all,labels = data[2]\n",
    "import random\n",
    "image = images_all[labels==4]\n",
    "idx = random.randint(0,image.shape[0])\n",
    "image = image[idx:idx+1]\n",
    "recon_image = f(image).reshape(28,28)\n",
    "plt.imshow(np.hstack((image.reshape(28,28),recon_image)),interpolation=\"nearest\",cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G,weights = get_filters()\n",
    "plt.axis('off')\n",
    "plt.imshow(np.hstack(np.hstack(weights.T.reshape((20,25,28,28)))),interpolation=\"nearest\",cmap=cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(np.hstack(np.hstack(G.T.reshape((5,8,28,28)))),interpolation=\"nearest\",cmap=cm.gray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
