{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Turing Machines (NTM)\n",
    "\n",
    "Alex Graves, Greg Wayne, Ivo Danihelka\n",
    "$$\n",
    "\\newcommand{\\memory}{\\mathbf{M}}\n",
    "\\newcommand{\\read}{\\mathbf{r}}\n",
    "\\newcommand{\\erase}{\\mathbf{e}}\n",
    "\\newcommand{\\add}{\\mathbf{a}}\n",
    "\\newcommand{\\weight}{\\mathbf{w}}\n",
    "\\newcommand{\\key}{\\mathbf{k}}\n",
    "\\newcommand{\\shift}{\\mathbf{s}}\n",
    "\\newcommand{\\Shift}{\\mathbf{S}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##`$ finger shawntan`\n",
    "(by the way, `finger` is totally the correct command to use here, `whoami` doesn't make sense at all.)\n",
    "```\n",
    "Login name: shawntan                    In real life: Tan Jing Shan,Shawn\n",
    "\n",
    "Occupation: Research Assistant @ SoC (Speech Recognition Group)\n",
    "\n",
    "- Graduated from NUS\n",
    "- Worked at Semantics3\n",
    "\n",
    "Research Interests: \n",
    "- Neural Networks\n",
    "- Machine Learning\n",
    "- Natural Language Processing\n",
    "- Artificial Intelligence\n",
    "```\n",
    "### PLEASE interrupt me if I'm not clear. It helps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Here's the plan\n",
    "\n",
    "1. Motivation behind doing this.\n",
    "2. The Model.\n",
    "3. Where things (have gone and) should go from here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Let's look at the tasks they'll make the NTM perform...\n",
    "1. **Copy**\n",
    "   Give a variable input length of binary vectors. Once I'm done, regurgitate everything I gave you.\n",
    "   \n",
    "2. **Repeat copy**\n",
    "   Give a variable input length of binary vectors. Once I'm done, I'll give you a number N. Regurgitate everything I gave you N times.   \n",
    "   \n",
    "3. **Associative Recall**\n",
    "   Give a sequence of items. When shown one item from the sequence, show the next item in line.\n",
    "   \n",
    "4. **Dynamic N-Grams**\n",
    "   \"Learning to learn\" an N-gram distribution estimator. Give you binary sequence, try to predict the next in line.\n",
    "   \n",
    "5. **Priority Sort**\n",
    "   Give a sequence of priority-symbol pairs, output is symbol sequence sorted by priority value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks (GRUs, LSTMs, they're all the same..)\n",
    "\n",
    "GRUs and LSTMs can perform these tasks, so why bother?\n",
    "\n",
    "My hunch?\n",
    "1. **Better generalisation** More unlikely for the network to just memorise familiar sequences from the training data.\n",
    "2. **Visualisation, introspection** Being able to visualise what the network is doing by limiting the actions it can take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Model\n",
    "\n",
    "Just think of it like this...\n",
    "<img src=\"images/controller.png\"/>\n",
    "* Controller: Multi-layer perceptron (MLP) or LSTM\n",
    "* Data structure: 128 x 20 array (128 is the number of 'slots')\n",
    "* Input: Depends on task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Read and Write heads\n",
    "\n",
    "The paper mentions these \"heads\" often. But never really says how many there are and what they do.\n",
    "1. I've assumed that there are separate read and write heads.\n",
    "2. The heads decide where the acton (read or write) will occur in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we read from memory?\n",
    "We need to read one row from the array.\n",
    "1. Should be able to figure out on its own where to read from.\n",
    "2. Should be able to take derivatives through the process.\n",
    "\n",
    "Think about how we use a one-hot vector to select a row from the matrix...\n",
    "\n",
    "$$\n",
    "\\left[\\begin{matrix}\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "1 \\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{matrix} \\right]^{\\top}\n",
    "\\underbrace{\\left[\\begin{matrix}\n",
    "r_{0,0} & \\cdots & r_{0,d}\\\\\n",
    "&\\vdots& \\\\\n",
    "r_{i,0} & \\cdots & r_{i,d} \\\\\n",
    "&\\vdots&\\\\\n",
    "r_{|V|,0} & \\cdots & r_{|V|,d}\n",
    "\\end{matrix} \\right]}_{C}\n",
    "= \\left[r_{i,0} ~~ \\cdots ~~ r_{i,d}\\right] = C_i\n",
    "$$\n",
    "\n",
    "Their approach? Construct a vector that sums to 1, and use that to select the correct row by multiplying. This is the vector that is repeatedly referred to as $\\mathbf{w}_t$ or $w_t(i)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we write to memory?\n",
    "\n",
    "The idea is still the same, but now we have 3 things, a write head $\\weight_t$, an erase vector $\\erase_t$, and an add vector $\\add_t$. \n",
    "\n",
    "The process is then to,\n",
    "1. \"Softly\" erase the relevant stuff from the row of memory of the previous time step\n",
    "   $$\\hat{\\memory_t} (i) \\leftarrow \\memory_{t-1}(i)\\left[1 - w_t(i)\\erase_t\\right]$$\n",
    "2. \"Softly\" add the new data to the row of the modified memory\n",
    "   $$\\memory_t(i) \\leftarrow \\hat{\\memory}_t(i) + w_t(i) \\add_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Key points:\n",
    "1. Notice that in order for backpropagation through time to work for these \"external memory\" structures, the state of the memory at _every_ time step has to be kept.\n",
    "2. This method of using the \"expected value\" or \"soft\" read and writes, is referred to as _attention_ in current deep learning literature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A word about the \"heads\"\n",
    "\n",
    "How to generate these $\\weight_t$ vectors?\n",
    "\n",
    "1. **Content-based**\n",
    "   * Have a way to lookup memory by content (e.g. nearest neighbour, cosine simiilarity...)\n",
    "   * Similar to a hash-table type of lookup\n",
    "   \n",
    "2. **Location-based**\n",
    "   * Perform a shift based on $\\weight_{t-1}$ \n",
    "   * Similar to a increment in pointer or an increment to an array index.\n",
    "   \n",
    "And it also needs to decide between using either option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Constructing heads\n",
    "\n",
    "<img src=\"https://blog.wtf.sg/wp-content/uploads/2014/10/Screenshot-from-2014-10-30-125051.png\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* $\\key$: For looking up values within memory (via use of a similarity function)\n",
    "* $\\beta$: \"Sharpening\" factor for lookup\n",
    "* $g$: For interpolating/switching between content lookup and location lookup\n",
    "* $\\shift$: Content lookup --- Shifting a step from $\\weight_{t-1}$\n",
    "* $\\gamma$: Attention \"sharpener\"\n",
    "$$\\begin{align} \n",
    "\\key &= \\hat{\\key} &\\\\ \n",
    "\\beta &= \\log\\left(e^{\\hat{\\beta}} + 1 \\right)  &\\Longrightarrow &\\beta > 0 \\\\ \n",
    "g &= \\sigma\\left(\\hat{g}\\right) &\\Longrightarrow & g \\in (0,1) \\\\ \n",
    "(\\shift)_i &= \\frac{\\exp((\\hat{\\shift})_i)}{\\sum_j \\exp((\\hat{\\shift})_j)} &\\Longrightarrow \n",
    "& (\\shift)_i \\in (0,1),\\sum_i (\\shift)_i = 1 \\\\ \n",
    "\\gamma &= \\log\\left(e^{\\hat{\\gamma}} + 1 \\right) + 1 &\\Longrightarrow & \\gamma \\geq 1 \\\\ \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### So... what does the controller look like?\n",
    "\n",
    "<img src=\"images/controller_nndiag.png\"/>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
